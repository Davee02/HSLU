{
 "cells": [
  {
   "attachments": {
    "png-clipart-cartoon-network-johnny-bravo-television-show-drawing-others-television-comics-thumbnail.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network for Text Generation\n",
    "\n",
    "This tutorial is about Natural Language Processing (NLP) with Recurrent Neural Networks (RNN). We will use an RNN to build a (generative) language model that can produce new text snippets of similar style as in the training corpus. Unfortunately, NLP projects always involve a lot of data preprocessing. RNN based models are successful for rather short text snippets. The more recent technique of transformer networks show much better performance and scale to much longer texts (e.g. BERT or GPT-2). But they would also need much more computational resources for training, of course, which is why we content ourselves with this rather simple RNN model.\n",
    "\n",
    "**Information:**\n",
    "> If you run into any problems regarding insufficient computing power or RAM, you can skip the training part and jump straight to the generation. Or, if you are interested, limit the dataset and try the training again.\n",
    "\n",
    "## Dating for Nerds\n",
    "\n",
    "Nerds are notoriously bad when it comes to dating. Therefore, we aim to provide a new service to these shy and hesitant people that suggest pickup lines. Deployed as a mobile app, our customers hopefully get good support in finding the love of their life. We already crawled a dataset of $18150$, hopefully, successful pickup lines from the internet. So, let us start developing the world's first MACHO BOT ...\n",
    "\n",
    "![png-clipart-cartoon-network-johnny-bravo-television-show-drawing-others-television-comics-thumbnail.png](attachment:png-clipart-cartoon-network-johnny-bravo-television-show-drawing-others-television-comics-thumbnail.png)\n",
    "\n",
    "Image: Johnny Bravo\n",
    "\n",
    "**Disclaimer:** \n",
    "\n",
    "> Some of the crawled pickup lines are vulgar and offensive. But we chose this example intentionally and will reflect on this at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Library for Fracebook word embeddings\n",
    "# pip install --upgrade fasttext\n",
    "import fasttext.util\n",
    "\n",
    "# Library for progress bars\n",
    "# pip install --upgrade tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Interactive elements for Jupyter notebooks\n",
    "from ipywidgets import interact\n",
    "\n",
    "# Make sure you have the right tensorflow version installed\n",
    "assert tf.__version__ >= \"2.1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphic Processing Unit (GPU)\n",
    "\n",
    "You can run the notebook without any additional computing resources. However, it will take much longer to train the network. Therefore we recommend to go through the code and skip the training of the network. But if you have access to some additional computing resources, such as a GPU, you can certainly train the network and try to make adjustments to it. You can run the next cell to make sure TensorFlow correctly initializes the GPU. If no GPU is found, there will not be any output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download pre-trained `fasttext` word embeddings for English. This can take up to one hour depending on your network speed. The downloaded model is almost 4GB large and will be stored on your harddisc. Our code skips the download if the file is already avaiable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fasttext.util.download_model(\"en\", if_exists=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Examination\n",
    "\n",
    "Load the crawled pickup lines dataset into a pandas dataframe. The dataset consists of four columns:\n",
    "* `text`: the pickup line\n",
    "* `url`: the origin of the pickup line\n",
    "* `text_len`: the number of characters of the pickup line\n",
    "* `text_word_count`: the number of words of the pickup line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"pickup_lines_dataset_cleaned.csv\", index_col=0, encoding=\"utf-8\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at some stats of the dataset, starting with the distribution of the text length. We can see that most pickup lines are in the range between 30-90."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "df.hist(column=[\"text_len\"], bins=100)\n",
    "plt.title(\"Distribution of text length\")\n",
    "plt.xlabel(\"text length\")\n",
    "plt.ylabel(\"count in ds\")\n",
    "plt.xlim([0, 400])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the next cell shows that most pickup lines contain between 7-16 words. This seems like a typical length of pickup lines to be generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "df.hist(column=[\"text_word_count\"], bins=100)\n",
    "plt.title(\"Distribution of word counts\")\n",
    "plt.xlim([0, 60])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "After examining the data, we must take care of ponctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the text (pickup lines) from the dataframe\n",
    "text = df[\"text\"]\n",
    "\n",
    "# Replace some punctation\n",
    "text = text.str.replace(\";\", \",\")\n",
    "text = text.str.replace(\"...\", \".\", regex=False)\n",
    "text = text.str.replace(\"…\", \".\", regex=False)\n",
    "text.head(n=3).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symbols we want to filter out\n",
    "unwanted_punctation = r\"'„“#$%&()*+-/:<=>@[\\]^_`{|}~\"\n",
    "# Symbols we want to keep\n",
    "wanted_punctation = r\",?!.\"\n",
    "\n",
    "# Add whitespace before wanted punctation\n",
    "for punctation in wanted_punctation:\n",
    "    text = text.str.replace(punctation, \" {}\".format(punctation))\n",
    "text.head(n=3).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers (`word_index`), and another for numbers to characters (`rev_index`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters=unwanted_punctation, lower=False)\n",
    "tokenizer.fit_on_texts(text.to_list())\n",
    "\n",
    "# dict: keys=word, value=index in bag-of-words\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# dict: keys=index in bag-of-words, value=word (used to turn bag-of-words into text)\n",
    "rev_index = {v:k for k,v in word_index.items()}\n",
    "\n",
    "# Print some dictionary entries for illustration\n",
    "for char,_ in zip(word_index, range(7)):\n",
    "    print(\"  {} -> {},\".format(repr(char), word_index[char]), end='')\n",
    "print(' ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we will now map a sentence to a numerical representation. Consider the sentence *Do you want to go out with me?*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Do you want to go out with me?\"\n",
    "sent_num = encoded = tokenizer.texts_to_sequences([sent])[0]\n",
    "print(\"Sentence '{}' mapped to '{}'\".format(sent, sent_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load embeddings from ``fasttext`` file and create embeddings matrix. The number of rows equals the number of words in the corpus (plus 1 for an out-of-vocabulary (OOV) token), the number of columns equals the dimension of the embedding vectors, i.e. $300$ in case of fasttext. Again, loading 4GB into memory can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_model(\"cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "for word in word_index.keys():\n",
    "    word_idx = word_index[word]\n",
    "    word_vec = ft.get_word_vector(word)\n",
    "    # Add word vec to the embedding matrix at the index of the word\n",
    "    embedding_matrix[word_idx] = word_vec\n",
    "    \n",
    "print('Embeddings matrix shape:', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a word, or a sequence of words, what is the most probable next word? This is the task we are going to train the model for. The input to the model will thus be a sequence of words, and we train the model to predict the output, the following word at each time step. To do this, we first create sequences of $n$ consecutive words, the first $n-1$ being the input and the last being the output (what the model will learn to predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We generate sequences of length 3, i.e. predict the third work from the two foregoing words\n",
    "SEQUENCE_LEN = 3\n",
    "\n",
    "sequences = []\n",
    "for line in text.tolist():\n",
    "    encoded = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(SEQUENCE_LEN, len(encoded)+1):\n",
    "        sequence = encoded[i-SEQUENCE_LEN:i]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "# Padding is only necessary if we have lines shorter than SEQUENCE_LEN (not the case here)\n",
    "sequences = np.array(pad_sequences(sequences, maxlen=SEQUENCE_LEN, padding=\"pre\"))\n",
    "print('There are {} sequences of length {} words'.format(sequences.shape[0], sequences.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset consisting of the first two word as input $X$ and the last (third) word as categorical output $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "print(\"Input shape: {0}, Output shape: {1}\".format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Models\n",
    "Now we define a function that takes the model, a trained tokenizer, a seed word (starting word of the pickup line) and a maximum number of words. The function then generates a sequence of words that follow the learned distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, tokenizer, max_length, seed_text, max_words):\n",
    "    result = seed_text\n",
    "    sentence_num = 0\n",
    "    for _ in range(max_words):\n",
    "        # Encode the text as integer and pad the sequence to the max length of the sequence\n",
    "        encoded = tokenizer.texts_to_sequences([result])[0]\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding=\"pre\")\n",
    "        \n",
    "        # Predict probabilities of all next words\n",
    "        y_probas = model.predict(encoded, batch_size=1024, verbose=0)[0]\n",
    "        # Get the corresponding classes (the actual words)\n",
    "        y_classes = np.arange(vocab_size)\n",
    "        # Sample the next word from the learned distribution\n",
    "        y_hat = np.random.choice(y_classes, p=y_probas)\n",
    "        \n",
    "        # Map predicted word index to word\n",
    "        out_word = rev_index[y_hat]\n",
    "        # Append newly sampled word to previous ones\n",
    "        result += \" \" + out_word\n",
    "        \n",
    "        # Check if sentence end and stop after two sentences\n",
    "        if out_word in [\".\", \"?\", \"!\"]:\n",
    "            sentence_num += 1\n",
    "            if sentence_num == 2:\n",
    "                break\n",
    "                \n",
    "    # Remove whitespace before wanted punctation\n",
    "    for punctation in wanted_punctation:\n",
    "        result = result.replace(\" {}\".format(punctation), punctation)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of all the first words for seeding the generator, sampled from all of the possible words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_words = text.str.capitalize().str.split(\" \").map(lambda x: x[0])\n",
    "print(seed_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a generator callback which is called after every epoch during training. This callback is supposed to generate a new sequence starting with a randomly sampled word from our `seed_words`. With this output, we can examine the learning behaviour of our model. It should start off with mostly random pickup lines and then briefly start to learn how to build realistic sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_and_print(epoch, logs):\n",
    "    seed_word = np.random.choice(seed_words)\n",
    "    generated = generate_seq(model, tokenizer, SEQUENCE_LEN-1, seed_word, 15)\n",
    "    print(\"Epoch {} - {}\\n\".format(epoch + 1, generated))\n",
    "    \n",
    "generator_callback = LambdaCallback(on_epoch_end=gen_and_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build the RNN model with LSTM cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, \n",
    "                    EMBEDDING_DIM, \n",
    "                    weights=[embedding_matrix], \n",
    "                    input_length=SEQUENCE_LEN-1, \n",
    "                    trainable=False))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have all the necessary building blocks, we can now compile and train the model using the created dataset $X$ and $y$. $X$ are two words which will be passed to the model, and it will learn to predict the third word $y$. When training the model using `fit()` we will pass it our custom `generator_callback`, which will generate a sequence after each epoch. This step will take some time, depending on the available hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "# Reduce the number of epochs when necessary. \n",
    "# Epochs = 200 gives reasonable performance; epochs = 3 is good for testing.\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(X, y, epochs=3, verbose=2, callbacks=[generator_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model and tokenizer. Always make sure you also save the tokenizer, otherwise your model would be useless since you can not translate the numerical outputs back into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"model.h5\"):\n",
    "    print(\"Saved model does not exist, saving current model.\")\n",
    "    model.save(\"model.h5\")\n",
    "    with open(\"tokenizer.pkl\", \"bw\") as f:\n",
    "        pickle.dump(file=f, obj=tokenizer)\n",
    "else:\n",
    "    print(\"Saved model exists, loading it.\")\n",
    "    model = load_model('model.h5')\n",
    "    with open('tokenizer.pkl', 'br') as f:\n",
    "        tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the trained Generator\n",
    "\n",
    "In the last step, we will use the trained model from above, or the saved model, to generate pickup lines. To do this, we can use the `generate_seq()` function again. You can play around with different seed words and sequence lengths. Maybe you should also retrain the model over night with more epochs. The runtime is fairly linear. Measure the time it takes for 3 epochs and calculate the expected waiting time for e.g. 200 epochs. Have fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_seq(model, tokenizer, SEQUENCE_LEN-1, \"I\", 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated pick up lines are pretty bad from a grammatical point of view (of course, also when considering the content). Most of them also do not make much sense, which can also be caused by the fact, that pickup lines mostly also don't make much sense :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On a more serious Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have heard about Aweful AI - here is a [website](https://github.com/daviddao/awful-ai) that collects all these stories of artificial intelligences turning mad. There are hundrets of such stories: the Microsoft chat bot became racists and antisemitic, the Google image captioning system maks black people as gorillas, the Amazon hiring application filters out women, etc. Why does this happen? Neural networks learn structures from data. When your data is as bad as our pickup lines dataset, then what else can you expect. This sounds like a problem easy to avoid - just make sure that the dataset is appropriate. Oh well, modern deep learning systems are trained on hundred of millions of images and text documents. Who on earth is going to check whether this data does not disadvantage minorities, etc. Training a real conversational agent (not the scriped chatbots from your e-banking system) requires a hell lot of data, and we cannot just use wikipedia for training as a chatbot requires conversational data in question-answer form. the is actually only one data source that is large enough for this: subscripted Hollywood movies. And what shall we expect when we train a neural net on *Hasta la vista, baby* ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
