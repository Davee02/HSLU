{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxj7_HDXCveU"
   },
   "source": [
    "# Generative Adversarial Network (GAN)\n",
    "This exercise focuses on generating Pokemons using a GAN. Since these types of neural networks are computationally expensive, we will use Google Colab for this exercise. There we have the option to run the notebook with a GPU, which speeds up the training.\n",
    "\n",
    "> To connect a GPU to the current Colab notebook, go to \"Runtime\" > \"Change runtime type\" and select \"GPU\" in the \"Hardware accelerator\" dropdown menu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2k7AAMRMCveU"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import zipfile\n",
    "import imageio\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import RMSprop\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JAmiehekM__"
   },
   "source": [
    "## Upload and extract the dataset\n",
    "The first step is to upload the Pokemon dataset to Google Colab. This can be done by opening the \"files\" pane on the left and clicking the upload button. Make sure to upload the dataset as a ZIP file and not as the previously extracted one. The extraction of the dataset is then done within Colab itself. Uploading the dataset can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RW7sRNELjyZ-"
   },
   "outputs": [],
   "source": [
    "# extract the pokemon data\n",
    "zip_ref = zipfile.ZipFile(\"pokemon-dataset.zip\", \"r\")\n",
    "zip_ref.extractall(\"pokemon-dataset/\")\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bzv9jpNRDOkn"
   },
   "source": [
    "## Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8_gsLYSCveW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64\n",
    "epochs = 30000\n",
    "\n",
    "image_size = 64, 64, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEB5iCZLsGcZ"
   },
   "source": [
    "Load the dataset from the extracted ZIP file. As a pre-processing step, the images get resized to `64x64` pixels to speed up the training and then normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J94hVTPksEF5",
    "outputId": "72bd3a5c-c2aa-42f3-8539-7babb6b64fa9"
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "pokemon_path = \"pokemon-dataset/pokemon_jpg/pokemon_jpg\"\n",
    "pokemon_files = np.array(sorted(os.listdir(pokemon_path)))\n",
    "x_train = [imageio.imread(os.path.join(pokemon_path, x)) for x in pokemon_files]\n",
    "x_train = np.asarray(x_train)\n",
    "\n",
    "# resize the data\n",
    "x_train = [Image.fromarray(np.uint8(img)).resize((64, 64)) for img in x_train]\n",
    "x_train = [np.asarray(img, dtype=np.float32) for img in x_train]\n",
    "x_train = np.asarray(x_train, dtype=np.float32)\n",
    "\n",
    "# normalize data\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "\n",
    "# check number of images\n",
    "print(str(x_train.shape[0]) + \" images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyuL2dDrsd2N"
   },
   "source": [
    "Let's look at some random images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "7z85rBwxIZM_",
    "outputId": "f31ff1ca-9920-4e93-d277-c4872db6c181"
   },
   "outputs": [],
   "source": [
    "# plot some images of the dataset to look at our training data\n",
    "image_indices = np.random.randint(0, x_train.shape[0] - 1, [12])\n",
    "real_images = x_train[image_indices]\n",
    "\n",
    "for i in range(12):\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    plt.imshow(real_images[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSlSoqkeI-Km"
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKlobgcetK0g"
   },
   "source": [
    "This function defines the **adaptive instance normalization**, a normalization technique used in StyleGAN and StyleGAN2. It aligns the mean and the variance of the content features with those of the style features. In other words, this technique applies the style features to the content features.\n",
    "\n",
    "$$ AdaIN(x,y) = \\sigma(y)\\Big(\\frac{x-\\mu(x)}{\\sigma(x)} \\Big) + \\mu(y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1RYN2W0CveW"
   },
   "outputs": [],
   "source": [
    "def AdaIN(x):\n",
    "  # normalize x[0] (image representation)\n",
    "  mean = K.mean(x[0], axis = [1, 2], keepdims = True)\n",
    "  std = K.std(x[0], axis = [1, 2], keepdims = True) + 1e-7\n",
    "  y = (x[0] - mean) / std\n",
    "  \n",
    "  # reshape scale and bias parameters\n",
    "  pool_shape = [-1, 1, 1, y.shape[-1]]\n",
    "  scale = K.reshape(x[1], pool_shape)\n",
    "  bias = K.reshape(x[2], pool_shape)\n",
    "  \n",
    "  # multiply by x[1] (GAMMA) and add x[2] (BETA)\n",
    "  return y * scale + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PdVDlFfundg"
   },
   "source": [
    "Next, the `g_block` and `d_block` are defined. They are the generator and discriminator blocks used in the network. Defining these layers as functions will make the code for the creation of the model much cleaner. \n",
    "\n",
    "The `g_block` follows an architecture similar to StyleGAN, which consists of    \n",
    "`Upsampling -> Convolution -> AdaIN -> Activation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4awdPj4bCveW"
   },
   "outputs": [],
   "source": [
    "def g_block(input_tensor, latent_vector, filters):\n",
    "  gamma = Dense(filters, bias_initializer = \"ones\")(latent_vector)\n",
    "  beta = Dense(filters)(latent_vector)\n",
    "  \n",
    "  out = UpSampling2D()(input_tensor)\n",
    "  out = Conv2D(filters, 3, padding = \"same\")(out)\n",
    "  out = Lambda(AdaIN)([out, gamma, beta])\n",
    "  out = Activation(\"relu\")(out)\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4_QdZenCveW"
   },
   "outputs": [],
   "source": [
    "def d_block(input_tensor, filters):\n",
    "  out = Conv2D(filters, 3, padding = \"same\")(input_tensor)\n",
    "  out = LeakyReLU(0.2)(out)\n",
    "  out = AveragePooling2D()(out)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Td60ii7Ju-A4"
   },
   "source": [
    "## Generator\n",
    "The generator network is responsible for generating a new image from a random input. The input to the generator first gets mapped to a latent space using three fully connected layers. Then this latent vector gets passed to four generator blocks, followed by the last convolution layer. This will then generate a `64x64x3` output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FdvDo7ETCveW",
    "outputId": "cd744dab-c80f-4018-efd9-7dcc790cc8d4"
   },
   "outputs": [],
   "source": [
    "# latent input\n",
    "latent_input = Input([64])\n",
    "\n",
    "# map latent input\n",
    "latent = Dense(64, activation = \"relu\")(latent_input)\n",
    "latent = Dense(64, activation = \"relu\")(latent)\n",
    "latent = Dense(64, activation = \"relu\")(latent)\n",
    "\n",
    "# reshape to 4x4x64\n",
    "x = Dense(4*4*64, activation = \"relu\")(latent_input)\n",
    "x = Reshape([4, 4, 64])(x)\n",
    "\n",
    "# size: 4x4x64\n",
    "x = g_block(x, latent, 64)\n",
    "# size: 8x8x64\n",
    "x = g_block(x, latent, 32)\n",
    "# size: 16x16x32\n",
    "x = g_block(x, latent, 16)\n",
    "# size: 32x32x16\n",
    "x = g_block(x, latent, 8)\n",
    "\n",
    "# size: 64x64x8, make RGB with values between 0 and 1\n",
    "image_output = Conv2D(3, 1, padding = \"same\", activation = \"sigmoid\")(x)\n",
    "\n",
    "# make Model\n",
    "generator = Model(inputs = latent_input, outputs = image_output)\n",
    "# model Summary\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cm_R-vmkwPwB"
   },
   "source": [
    "## Discriminator\n",
    "The discriminator is responsible for distinguishing between images that are generated or real. In other words, it predicts the class (`real` or `fake`) an image belongs to. The discriminator consists of four `d_block`, followed by a convolution. Then the output gets flattened and passed to a final fully connected layer with just one neuron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "osXOfONyCveX",
    "outputId": "aab874ea-106b-4a4d-b8c5-a49eee61f7cd"
   },
   "outputs": [],
   "source": [
    "# image input\n",
    "image_input = Input([64, 64, 3])\n",
    "\n",
    "# size: 64x64x3\n",
    "x = d_block(image_input, 8)\n",
    "# size: 32x32x8\n",
    "x = d_block(x, 16)\n",
    "# size: 16x16x16\n",
    "x = d_block(x, 32)\n",
    "# size: 8x8x32\n",
    "x = d_block(x, 64)\n",
    "\n",
    "# size: 4x4x64\n",
    "x = Conv2D(128, 3, padding = \"same\")(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "# 1-dimensional Neural Network\n",
    "class_output = Dense(1)(x)\n",
    "\n",
    "# make Model\n",
    "discriminator = Model(inputs = image_input, outputs = class_output)\n",
    "# model summary\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzYzIbmN2t_d"
   },
   "source": [
    "## Build the Discriminator and the Generator\n",
    "The discriminator and generator need to be built together because their training is dependent on each other. Thus, we need to ensure that when the discriminator gets trained, the generator doesn't simultaneously get trained and vice versa.\n",
    "\n",
    "The discriminator will be trained by classifying the validity of the real image and then the validity of the fake image. The goal is to ensure that the discriminator can distinguish between real and fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCJgcx8sCveX"
   },
   "outputs": [],
   "source": [
    "# build network to train the discriminator.\n",
    "# discriminator will train, but Generator won't train.\n",
    "for layer in discriminator.layers:\n",
    "  layer.trainable = True\n",
    "    \n",
    "for layer in generator.layers:\n",
    "  layer.trainable = False \n",
    "    \n",
    "# get real image\n",
    "real_image = Input([64, 64, 3])\n",
    "# discriminator classifies\n",
    "validity_real = discriminator(real_image)\n",
    "\n",
    "# get latent input\n",
    "latent_input = Input([64])\n",
    "# generate an image\n",
    "fake_image = generator(latent_input)\n",
    "# discriminator classifies\n",
    "validity_fake = discriminator(fake_image)\n",
    "\n",
    "# create and compile the model\n",
    "DiscriminatorModel = Model(inputs = [real_image, latent_input], \n",
    "                           outputs = [validity_real, validity_fake, validity_real])\n",
    "DiscriminatorModel.compile(optimizer = RMSprop(lr = 0.0002),\n",
    "                           loss = [\"mean_squared_error\", \"mean_squared_error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pR0fr6ey30oH"
   },
   "source": [
    "The generator creates an image from random noise. This generated image will then be evaluated using the discriminator. The goal is that the generator can create images, which the discriminator can not distinguish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyphS9sICveX"
   },
   "outputs": [],
   "source": [
    "# build network to train the generator.\n",
    "# discriminator won't train, but Generator will train.\n",
    "for layer in discriminator.layers:\n",
    "  layer.trainable = False\n",
    "    \n",
    "for layer in generator.layers:\n",
    "  layer.trainable = True\n",
    "\n",
    "# get latent input\n",
    "latent_input = Input([64])\n",
    "# generate an image\n",
    "fake_image = generator(latent_input)\n",
    "# discriminator classifies\n",
    "validity = discriminator(fake_image)\n",
    "\n",
    "# create and compile the model\n",
    "GeneratorModel = Model(inputs = latent_input, outputs = validity)\n",
    "GeneratorModel.compile(optimizer = RMSprop(lr = 0.0002), loss = \"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXtc0qja4Kld"
   },
   "source": [
    "## Training the model\n",
    "After the generator and discriminator have been built, the GAN can finally be trained. The training happens in two steps. First, the discriminator gets trained and then, as a second step, the generator. The entire training will take around ~35min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0RJn-aECveX",
    "outputId": "6f3ae953-2bd3-46af-d4cd-e55301116fb7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# statistics of the GAN training \n",
    "d_loss = [] # discriminator loss\n",
    "g_loss = [] # generator loss\n",
    "predictions = [] # predictions at each epoch\n",
    "\n",
    "# train the models in a loop\n",
    "for i in tqdm(range(epochs)):\n",
    "  clear_output(wait=True)\n",
    "\n",
    "  # get labels\n",
    "  real_labels = np.ones([batch_size, 1])\n",
    "  fake_labels = np.zeros([batch_size, 1])\n",
    "  dummy_labels = np.ones([batch_size, 1])\n",
    "\n",
    "  # train discriminator\n",
    "  # get images and latent vectors\n",
    "  image_indices = np.random.randint(0, x_train.shape[0] - 1, [batch_size])\n",
    "  real_images = x_train[image_indices]\n",
    "  latent_vectors = np.random.normal(0.0, 1.0, [batch_size, 64])\n",
    "  # train\n",
    "  loss = DiscriminatorModel.train_on_batch([real_images, latent_vectors], \n",
    "                                           [real_labels, fake_labels, dummy_labels])\n",
    "  d_loss.append(loss[1]/2 + loss[2]/2)\n",
    "\n",
    "  # train generator\n",
    "  # get latent vectors\n",
    "  latent_vectors = np.random.normal(0.0, 1.0, [batch_size, 64])\n",
    "  # train using opposite labels\n",
    "  loss = GeneratorModel.train_on_batch(latent_vectors, real_labels)\n",
    "  g_loss.append(loss)\n",
    "\n",
    "  if i % 100 == 0:\n",
    "    # sample of generator\n",
    "    latent_vectors = np.random.normal(0.0, 1.0, [16, 64])\n",
    "    predictions.append(generator.predict(latent_vectors)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STn2pKmO5Egx"
   },
   "source": [
    "To ensure the model has learned something during training, we can plot both the discriminator and generator loss. Since they both compete with each other, the optimal value would be if both of the losses are around 0.5. This would mean that the discriminator can not distinguish between the real and fake, the best it can do would be to guess. \n",
    "\n",
    "We can see that this optimal case hasn't occured in our simple example. The generator is much worse than the discriminator. One way to tackle this problem would be to try out a different architecture or to implement an additional loss function. For example \"gradient penalty loss\", which would prevent the model from collapsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "D7AJkOMnCveX",
    "outputId": "2ac86d74-1083-4404-8f74-05c85d50da47",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.plot(d_loss, label=\"discriminator\")\n",
    "plt.plot(g_loss, label=\"generator\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2K2JeHRY4ynq"
   },
   "source": [
    "After we have successfully trained our network, it's time to generate some images with it and look how good they are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "tvOQaq98CveX",
    "outputId": "866d9580-226c-4c6f-e128-39c5a480f27d"
   },
   "outputs": [],
   "source": [
    "# show samples\n",
    "latent_vectors = np.random.normal(0.0, 1.0, [12, 64])\n",
    "fake_images = generator.predict(latent_vectors)\n",
    "\n",
    "for i in range(12):\n",
    "  plt.subplot(3, 4, i+1)\n",
    "  plt.imshow(fake_images[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGZkG_rj5oPZ"
   },
   "source": [
    "We can also visualize the generated images a bit more fancy by creating a grid of fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xx5Cw9CXCveX"
   },
   "outputs": [],
   "source": [
    "latent_vectors = np.random.normal(0.0, 1.0, [2000, 64])\n",
    "fake_images = generator.predict(latent_vectors)\n",
    "\n",
    "cols = []\n",
    "for i in range(200):\n",
    "    cols.append(np.concatenate(fake_images[i*10:i*10+10], axis = 0))\n",
    "\n",
    "grid = np.concatenate(cols, axis = 1)\n",
    "x = Image.fromarray(np.uint8(grid*255))\n",
    "x.save(\"prediction_grid.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-FB5rch5z4i"
   },
   "source": [
    "Since we have generated an image after every 100th epoch, we can now visualize the training process as a GIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnKQIiG1CveX"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import shutil\n",
    "converted_predictions = []\n",
    "for img in predictions:\n",
    "  converted_predictions.append(np.uint8(img*255))\n",
    "imageio.mimsave(\"generation.gif\", converted_predictions)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Generative adversarial network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
