{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Feature Engineering with Pre-Trained Neural Networks\n",
    "\n",
    "Deep convolutional neural networks (CNN) are pre-trained on public datasets with millions of images. For example, the famous [ImageNet](http://www.image-net.org) catalogue consists of not less than 24 million images. Such pre-trained networks (i.e. their architecture and weights) are made avaiable for transfer learning (specialization to a specific domain by re-training, see later in this course) or as feature extractors for other machine learning purposes. The architecture of general-purpose CNNs is such that they first learn a compact encoding or representation of input images (e.g. numeric vector of 2048 dimensions) before they map the encoded image to the final category. A CNNs can thus be used for feature engineering by feeding it new images and extract their encodings from one of the inner layers. In this exercise, we use such a pre-trained CNN to encode images from a separate dataset and use the image feature vectors for image similarity computation much like it would be implemented in an image retrieval application with not keywords or captions available. The dataset used in the exercise consists of a small subset from the [Corel](https://sites.google.com/site/dctresearch/Home/content-based-image-retrieval) image database. It consists of 10 concept groups of images where each is composed by 100 images. The dataset is publicly available on [Kaggle](https://www.kaggle.com/elkamel/corel-images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "# Run pip install --upgrade tensorflow_hub if necessary\n",
    "import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Library for progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Interactive elements for Jupyter notebooks\n",
    "from ipywidgets import interact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(os.getcwd()) / \"dataset\"\n",
    "\n",
    "# Where to load train and test images from\n",
    "train_path  = dataset_path / \"training_set\"\n",
    "test_path   = dataset_path / \"test_set\"\n",
    "\n",
    "# Where to save image emeddings\n",
    "embeds_path = dataset_path / \"embeddings\"\n",
    "\n",
    "# print(\"Default dataset path: {}\".format(dataset_path))\n",
    "# print(\"Train path: {}\".format(train_path))\n",
    "# print(\"Test path: {}\".format(test_path))\n",
    "# print(\"Embeds path: {}\".format(embeds_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method loads an image from a given path using tensorflow methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((img_height, img_width))\n",
    "    img = np.array(img)\n",
    "    img = img / 255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method plots 9 random images in a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nine_random_images(images, labels):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # Loop over the dataset by taking one image and label at a time\n",
    "    for i in range(9):\n",
    "        img_index = np.random.randint(0, len(images))\n",
    "        # We display a 3x3 grid of images\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[img_index], cmap=\"gray\")\n",
    "        plt.title(labels[img_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method returns all images in a given path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_image_file_names(path):\n",
    "    return_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for name in files:\n",
    "            if name.split(\".\")[-1] == \"jpg\":\n",
    "                file_path = os.path.join(root, name)\n",
    "                folder_name = root.split(\"/\")[-1]\n",
    "                return_files.append([file_path, folder_name])\n",
    "    return return_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Generate Image Feature Vectors\n",
    "\n",
    "To calculate similarty between images, they must first be encoded into a feature vector. There are many ways of doing this. The computer vision community for example proposed Scale-Invariant Feature Transform (SIFT), Speeded-Up Robust Features (SURF) or Features from Accelerated Segment Test (FAST). However, more recent approached use pre-trained CNNs as feature extractors as they can convincingly extract complex high-level features from images. For this purpose, we first load a pre-trained CNN from the TensorFlow hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pre-Trained CNN\n",
    "\n",
    "There are different pre-trained CNNs avaiable on the [TensorFlow hub](https://tfhub.dev/s?module-type=image-feature-vector). For this exercise we use a pre-trained Inception v3 model and extract the features from the last dense layer of the network. In case you want to try a different model, just make sure that you pass the correct input shape to the model. To load a pre-trained model, you only need the respective TensorFlow Hub URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which model to load\n",
    "clf_name = \"Inception_v3\"\n",
    "embedder_model = \"https://tfhub.dev/google/imagenet/inception_v3/feature_vector/4\"\n",
    "\n",
    "# Shape of the model input\n",
    "img_height   = 299\n",
    "img_width    = 299\n",
    "img_channels = 3\n",
    "\n",
    "# Load model\n",
    "embedder = tf.keras.Sequential()\n",
    "embedder.add(hub.KerasLayer(embedder_model, trainable=False))\n",
    "\n",
    "# Specify input shape (batch, height, width, channels)\n",
    "embedder.build([None, img_height, img_width, img_channels])  \n",
    "\n",
    "# Print model information\n",
    "embedder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image Dataset\n",
    "\n",
    "The selected Inception v3 model thus generates 2048 dimensional feature vectors. Now we pass every image from our dataset to the model and extract the feature vectors. As you can imagine, this will takes some time. We show a progress bar and save this the extracted feature vectors to a file. For future experiments, you can just load the file and do not need to re-run the complete encoding process. Our dataset is split into *training* and *testing* images to be loaded separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files  = return_image_file_names(train_path)\n",
    "train_labels = [labels[1] for labels in train_files]\n",
    "print('{} training image paths found'.format(len(train_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = return_image_file_names(test_path)\n",
    "test_labels = [labels[1] for labels in test_files]\n",
    "print('{} test image paths found'.format(len(test_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.array([load_image(str(f)) for f, _ in train_files])\n",
    "print(\"{} training images loaded\".format(len(train_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = np.array([load_image(str(f)) for f, _ in test_files])\n",
    "print(\"{} test images loaded\".format(len(test_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the images: {}\".format(train_images[0].shape))\n",
    "\n",
    "# Make sure that the shapes of the images match the model input\n",
    "assert train_images[0].shape == (img_height, img_width, img_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us display 9 randomly selected training images along with their label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nine_random_images(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us extract the feature vector for a single image, e.g. image 100, for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = train_images[100]\n",
    "\n",
    "print('Dimensionality of a single image is:\\t {}'.format(image.shape))\n",
    "\n",
    "# Add one dimension to the front to accomodate model input format\n",
    "image = image[np.newaxis, ...]\n",
    "\n",
    "print('Dimensionality of the image is now:\\t {}'.format(image.shape))\n",
    "\n",
    "# Extract feature vector from CNN\n",
    "vec = embedder.predict(image)\n",
    "\n",
    "print('Dimensionality of feature vector is:\\t {}'.format(vec.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we encode the entire dataset of training and test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeds = []\n",
    "test_embeds  = []\n",
    "\n",
    "# Extract feature vector for training images\n",
    "for i in tqdm(range(len(train_images))):\n",
    "    train_embeds.append(embedder.predict(train_images[i][np.newaxis, ...]).squeeze())\n",
    "    \n",
    "# Extract feature vector for test images\n",
    "for i in tqdm(range(len(test_images))):\n",
    "    test_embeds.append(embedder.predict(test_images[i][np.newaxis, ...]).squeeze())\n",
    "    \n",
    "train_embeds = np.asarray(train_embeds)\n",
    "test_embeds  = np.asarray(test_embeds)\n",
    "\n",
    "print(\"Shape embeddings train:\\t {}\".format(train_embeds.shape))\n",
    "print(\"Shape embeddings test:\\t {}\".format(test_embeds.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step we will save the embedded dataset to our local drive, using the `np.savez()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding_file_name = \"embeds_train_{}.npz\".format(clf_name)\n",
    "train_persisted_embeds = Path(dataset_path / embeds_path / train_embedding_file_name)\n",
    "np.savez(train_persisted_embeds, embeds=train_embeds, info=train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding_file_name = \"embeds_test_{}.npz\".format(clf_name)\n",
    "test_persisted_test = Path(dataset_path / embeds_path / test_embedding_file_name)\n",
    "np.savez(test_persisted_test, embeds=test_embeds, info=test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Image Similarity\n",
    "\n",
    "We will now use the image feature vectors to compute similarity between images. As a first step, we load the embeddings from the files again using the `np.load()` function. For your projects we recommend to split part 1 and partb 2 over different notebooks as for larger image seits the extraction of feature vectors takes a lot of time. However, you can speed up when you have access to GPU resources of course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeds_saved = np.load(dataset_path / embeds_path / train_embedding_file_name)\n",
    "train_embeds = train_embeds_saved[\"embeds\"]\n",
    "train_info = train_embeds_saved[\"info\"]\n",
    "train_names = [f for f, _ in train_info]\n",
    "\n",
    "test_embeds_saved = np.load(dataset_path / embeds_path / test_embedding_file_name)\n",
    "test_embeds = test_embeds_saved[\"embeds\"]\n",
    "test_info = test_embeds_saved[\"info\"]\n",
    "test_names = [f for f, _ in test_info]\n",
    "\n",
    "print(\"Train shape:\\t {0}\\nTest shape:\\t {1}\".format(train_embeds.shape, test_embeds.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use k-nearest neighbors (k-NN) along with the cosine distance to obtain the $k=3$ most similar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = NearestNeighbors(n_neighbors=3, metric='cosine')\n",
    "\n",
    "# Fit k-NN on the training set\n",
    "knn.fit(train_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the trained k-NN model to make predictions on the unseen test set and plot the three most similar images along with their respective distance to the input image and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_names = [os.path.relpath(x, dataset_path) for x in test_names]\n",
    "\n",
    "@interact(image_name=selection_names)\n",
    "def show_nearest_neighbour(image_name):\n",
    "    idx_test_image = selection_names.index(image_name)\n",
    "    distances, indices = knn.kneighbors([test_embeds[idx_test_image]])\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1,len(indices[0])+1,1)\n",
    "    plt.imshow(load_image(dataset_path / image_name))\n",
    "    plt.title(\"test image: {}\".format(test_info[idx_test_image][1]))\n",
    "    \n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        plt.subplot(1,len(indices[0])+1,i+2)\n",
    "        plt.imshow(load_image(train_names[idx]))\n",
    "        plt.title(\"dist: {0:.2f}, lbl: {1}\".format(distances[0][i], train_info[idx][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the first image comes from the test set, to which the k-NN model did not have access. The following three images are the three nearest neighbors from the training set with respect to the cosine distance calculated over the feature vectors extrected from the CNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
