{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeeec2e0",
   "metadata": {},
   "source": [
    "# A Step-by-Step Guide to Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis (PCA) is an important unsupervised learning algorithm used for dimensionaly reduction and measurement of redundany in data. The maths behind PCA involves eigenvalues, eigenvectors, eigen-decomposition and other elaborate concepts. This poses a challenge especially to those students who could not yet take an introiductory class to linear algebra. In this notebook we implement PCA from scratch for a very simple dataset with only 10 records and 5 features. It is thought to be a step-by-step demystification of PCA. \n",
    "\n",
    "Â© Marc Pouly & Reza Kakooee, Algorithmic Business Research Lab, HSLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be88da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33f8027",
   "metadata": {},
   "source": [
    "# The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Price': {0: 36.1, 1: 15.8, 2: 33.9, 3: 12.5, 4: 23.3, 5: 21.5, 6: 11.8, 7: 29.1, 8: 37.7, 9: 17.5}, \n",
    "    'Seats': {0: 8, 1: 4, 2: 6, 3: 4, 4: 6, 5: 6, 6: 4, 7: 6, 8: 6, 9: 4}, \n",
    "    'Horsepower': {0: 210, 1: 141, 2: 200, 3: 90, 4: 178, 5: 160, 6: 110, 7: 172, 8: 172, 9: 140}, \n",
    "    'Mileage': {0: 1840, 1: 2090, 2: 2335, 3: 3250, 4: 2385, 5: 2045, 6: 2435, 7: 2280, 8: 2535, 9: 2610}, \n",
    "    'Passengers': {0: 6, 1: 6, 2: 5, 3: 4, 4: 4, 5: 5, 6: 5, 7: 5, 8: 6, 9: 4}})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b450e013",
   "metadata": {},
   "source": [
    "The dataset contains 10 samples and 5 features with different units. Some features like `Mileage` include large numbers, while some features like `Seats` and `Passengers` have small values. Let us look deeper into the stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f07bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c75c2",
   "metadata": {},
   "source": [
    "The features have `non-zero mean`, and most of them have `std > 1`. For applying `PCA`, we first need to standardize the data such that each feature has `zero mean` and `unit variance`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ea5deb",
   "metadata": {},
   "source": [
    "# Standardizing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ddd1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data such that mean becomes 0\n",
    "df = df - df.mean()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08257d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data such that standard deviation becomes 1\n",
    "df = df / df.std()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6b2771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the stats\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8a5c14",
   "metadata": {},
   "source": [
    "Now all features have `zero mean` and `unit variance (std)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfe56e6",
   "metadata": {},
   "source": [
    "# The Covariance Matrix of $X$\n",
    "\n",
    "The idea of PCA is to find a transformation matrix `P` that transforms the dataset `X` into another dataset `Y = PX` such that the covariance matrix of `Y` is a diagonal matrix. In a diagonal matrix, all off-diagonal elements are zero, which means that we removed all redundancy among features. Let us look at the covariance matrix of `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70740a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance matrix calculateed with pandas\n",
    "covX = df.cov()\n",
    "\n",
    "sns.heatmap(covX, xticklabels=covX.columns, yticklabels=covX.columns, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7c5ae2",
   "metadata": {},
   "source": [
    "As expected, off-diagonal elements are not zero. We obtain the same result (up to a constant) by calculating `XX^T`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ed784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance matrix calculated by XX^T\n",
    "X = df.values.T\n",
    "D = X @ X.T\n",
    "\n",
    "sns.heatmap(1/(len(df)-1) * D, xticklabels=covX.columns, yticklabels=covX.columns, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556d177",
   "metadata": {},
   "source": [
    "# Extract  `Eigenvalues` and  `Eigenvectors` of $X X^T$\n",
    "\n",
    "The PCA transformation is obtained if we set $P^T = E$, where the columns of $E$ are the eigenvectors of $X X^T$.\n",
    "We therefore compute the eigenvectors of $D = X X^T$ with a linear algebra Python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578b50b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigs are eigenvalues, E are eigenvectors\n",
    "eigs, E = scipy.linalg.eigh(D)\n",
    "\n",
    "eigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4e6263",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sort the eigenvalues and eigenvectors array in decending order with respect to eigenvalues\n",
    "idx = eigs.argsort()[::-1]   \n",
    "eigs = eigs[idx]\n",
    "E = E[:,idx]\n",
    "\n",
    "eigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba8a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e386634",
   "metadata": {},
   "source": [
    "# Build the Transformation Matrix \n",
    "\n",
    "The transformation matrix P is made from the eigenvectors of $D$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2b2fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = E.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b26df",
   "metadata": {},
   "source": [
    "# Transform the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474d20ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = (P @ X).T\n",
    "\n",
    "## Convert Y back into a dataframe\n",
    "Y_df = pd.DataFrame(Y, columns=[f'PC_{i+1}' for i in range(Y.shape[1])])\n",
    "\n",
    "Y_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5a3608",
   "metadata": {},
   "source": [
    "Let us see if the covariance matrix is really a diagonal matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3342d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "covY = Y_df.cov()\n",
    "\n",
    "sns.heatmap(covY, xticklabels=covX.columns, yticklabels=covX.columns, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee33d78",
   "metadata": {},
   "source": [
    "Note, the values off the diagonal are zero; $10^{-16}$ is super close to zero; this is a phenomenon of numeric rounding and finite precision on computers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425c5b70",
   "metadata": {},
   "source": [
    "# Variance Ratio\n",
    "\n",
    "For dimensionality reduction, we are interested in those principal components with the highest variance (= eigenvalue). Let us calculate the variance ratio to check how many principal components we need to keep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba185d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_ratio_ = eigs / sum(eigs)\n",
    "\n",
    "explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bbca94",
   "metadata": {},
   "source": [
    "For example, the first dimension (eigenvector) explains 70% of the variance in the data. Here is the same information as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0558c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "xx = np.array(list(range(len(explained_variance_ratio_))))+1\n",
    "plt.plot(xx, explained_variance_ratio_, '-bo')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014679b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us keep only the first two ones, as seems they are much bigger than the rest.\n",
    "K = 2\n",
    "\n",
    "info_ratio = sum(explained_variance_ratio_[0:K])/sum(explained_variance_ratio_)*100\n",
    "\n",
    "print(f\"The first two principal components contain about {info_ratio:.2f}% of the whole information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5148cf0c",
   "metadata": {},
   "source": [
    "# Projection to 2D\n",
    "\n",
    "We drop `PC_3`, `PC_4`, and `PC_5` from the `Y_df` and visualize the data projected to 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75a1582",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = Y_df.copy()\n",
    "\n",
    "for col in ['PC_3', 'PC_4', 'PC_5']:\n",
    "    target_df = target_df.drop(col, axis=1)\n",
    "    \n",
    "target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(target_df, x=\"PC_1\", y=\"PC_2\")\n",
    "fig.update_layout(width = 500, height = 500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa428773",
   "metadata": {},
   "source": [
    "Observe that the data is spread out widely along the x-axis, and less along the y-axis, emphasizing the importance of the 1st principal component over the 2nd. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b85fb6",
   "metadata": {},
   "source": [
    "# Verification\n",
    "\n",
    "Let us verify our results by comparing with the `scikit-learn` implementation of PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130fd955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(df)\n",
    "\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab081c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_values = np.arange(pca.n_components_) + 1\n",
    "\n",
    "plt.plot(PC_values, pca.explained_variance_ratio_, 'ro-', linewidth=2)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d5407",
   "metadata": {},
   "source": [
    "`Scikit-Learn` results are consistent with our calculations.\n",
    "\n",
    "We hope we could successfully demystify PCA. It is not that difficult :-) \n",
    "\n",
    "Cheers\n",
    "\n",
    "Marc & Reza"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
