{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "In this exercise, we will take a closer look at feature engineering. It is an essential skill for every data scientist since it gives a performance boost to the machine learning model, which we will see shortly. Most features can be created by simple tricks or creative thinking and by taking a close look at the data.\n",
    "\n",
    "The exercise is split into four distinct parts:\n",
    "* traditional feature engineering, as it is often used in the industry\n",
    "* using the kernel trick to create some clever features\n",
    "* feature engineering for images\n",
    "* advanced feature engineering for stock price data\n",
    "\n",
    "*Note:* Although the topic of classification is covered in later lectures, simple classifiers were used in this exercise. However, these are only used to visualize the difference in using feature engineering. Hence, you can disregard the code containing classifiers and purely interpret the results.\n",
    "\n",
    "For this exercise, we will need the `yfinance` package, which is used to retrieve stock price data from yahoo finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import random\n",
    "import scipy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "#import yfinance as yf\n",
    "\n",
    "from skimage import io\n",
    "from skimage import data as skimage_data\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "# define our random seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Feature Engineering\n",
    "In the first part of the exercise, we will look at traditional feature engineering, which means that we will use domain knowledge to create additional relevant features. These features should then increase the predictive powers of a learning algorithm.\n",
    "\n",
    "We will use the [Titanic Kaggle dataset](https://www.kaggle.com/c/titanic) to showcase the benefits of using feature engineering. This dataset consists of information about the passengers of the Titanic as well as whether they survived or not. The goal is to predict if a passenger survived or not.\n",
    "\n",
    "We will create two datasets, the first one is just the raw titanic dataset, with minimal additions such as imputing missing values. And the second one will contain some additional features we created. Then we will train a model on both datasets and compare them on the same test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import the data\n",
    "df = pd.read_csv('titanic/train.csv')\n",
    "\n",
    "# store the target variable \n",
    "survived = df['Survived']\n",
    "\n",
    "# drop the target variable\n",
    "data = df.drop(['Survived'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are missing values in the dataset, we will first impute them by the median for the `Age` and the `Fare`. For `Embarked` we will fill the missing values by using the mode of the feature, which corresponds to the most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values for Age, Fare, Embarked\n",
    "data['Age'] = data['Age'].fillna(data['Age'].median())\n",
    "data['Fare'] = data['Fare'].fillna(data['Fare'].median())\n",
    "data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode())\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we copy the original dataset, so that we can then see the difference in performance when using feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transf = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first feature we will create is the passengers titles, which is available in the `Name`. The title can be seen for example in record 4, _Allen, Mr. William Henry_, where the name also includes his title. We will assume that the title will be beneficial for the algorithm to know, because it could indicate something about their survival in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the titles from the names and add them as a new column\n",
    "data_transf['Title'] = data_transf['Name'].apply(lambda x: re.search('([A-Z][a-z]+)\\.', x).group(1))\n",
    "data_transf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will visualize the frequency of each title in the dataset, to visualy inspect our newly created feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "data_transf['Title'].hist()\n",
    "plt.xlabel('Title')\n",
    "plt.ylabel('count')\n",
    "plt.title('Countplot of title')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are a lot of different titles in the dataset. To make the feature even better, we will combine some of the titles to one title, such as `Mlle` and `Ms` with `Miss`. We can combine them because some of them are the same title but just in a different language. Next, we will also create a bucket for all the other titles, which we will call `Special`, where we then add titles such for example `Major`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transf['Title'] = data_transf['Title'].replace({'Mlle':'Miss', 'Mme':'Mrs', 'Ms':'Miss'})\n",
    "data_transf['Title'] = data_transf['Title'].replace(['Don', 'Dona', 'Rev', 'Dr', 'Major', \n",
    "                                       'Lady', 'Sir', 'Col', 'Capt', 'Countess', 'Jonkheer'], 'Special')\n",
    "\n",
    "# plot the titles\n",
    "plt.figure(figsize=(15, 5))\n",
    "data_transf['Title'].hist()\n",
    "plt.xlabel('Title')\n",
    "plt.ylabel('count')\n",
    "plt.title('Countplot of title after bucketing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have engineered our first feature, we will look at our data again and think of other creative ways of creating some additional features. We can see that the `Cabin` column contains some `NaN` values, which is reasonable since not every passenger on the titanic had a cabin. The information if a passenger had a cabin or not can give us some additional information whether they survived or not. Therefore we will create a new feature called `Has_Cabin`, which is `True` if they had a cabin and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `.isnull()` method, which will return `True` if `Cabin` is empty, which means the passenger did not have a cabin. And since we want to know if someone had a cabin or not, we need to flip the result by using tilde `~`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transf['Has_Cabin'] = ~data_transf['Cabin'].isnull()\n",
    "data_transf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will drop some of the columns since we won't be using them here in our example. However, there still might be a lot of useful information in them, especially in the `Cabin` column. But for our example, we can discard it.\n",
    "* `Cabin`, because we already extracted the feature `Has_Cabin`\n",
    "* `Name`, because we already extracted the feature `Title`\n",
    "* `PassengerId` and `Ticket`, because they probably do not tell us something about their survival status, since they are both just ID's\n",
    "* `SibSp` and `Parch` could be used to get the family size, but in our exercise, we will ignore that fact and drop these features\n",
    "\n",
    "We will drop these columns for both our transformed and original dataset to reduce the feature count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Cabin', 'Name', 'PassengerId', 'Ticket', 'SibSp', 'Parch'], axis=1, inplace=True)\n",
    "data_transf.drop(['Cabin', 'Name', 'PassengerId', 'Ticket', 'SibSp', 'Parch'], axis=1, inplace=True)\n",
    "data_transf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important aspect of feature engineering is numerical binning, which means that we will create a fixed amount of bins and then assign the numbers to their corresponding bin. The idea is that often, numerical data without binning them contains many fluctuations and therefore doesn't reflect the actual pattern in the data, because it may also contain noise. We will use pandas `.qcut()` method for this and bin our numerical data (`Age` and `Fare`) in four buckets. And since we extracted all the information for both of the features, we can safely drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transf['CatAge'] = pd.qcut(data_transf['Age'], q=4, labels=False)\n",
    "data_transf['CatFare'] = pd.qcut(data_transf['Fare'], q=4, labels=False)\n",
    "\n",
    "data_transf.drop(['Age', 'Fare'], axis=1, inplace=True)\n",
    "\n",
    "data_transf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training our model we need to handle the categorical data in our dataset. We will use `.get_dummies()` from pandas to convert the categorical features to numerical ones. One nice benefit of this function is, that for each categorical feature it will decide if it is more suited to use one hot encoding or label encoding, so that we don't need to think about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we will also do the conversion for the orignial dataset, \n",
    "# because otherwise we would't be able to use it\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "data_transf = pd.get_dummies(data_transf, drop_first=True)\n",
    "data_transf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the models, we will need to split the data into a training and test set. And since we do not do any hyperparameter tuning, we don't need a validation set here. We will do the split for both the transformed and the original dataset. And since we are using the same split seed, we will get the same test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split original dataset\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    data, survived, test_size=0.15, random_state=SEED)\n",
    "\n",
    "# split transformed dataset\n",
    "X_t_train, X_t_test, y_t_train, y_t_test = sklearn.model_selection.train_test_split(\n",
    "    data_transf, survived, test_size=0.15, random_state=SEED)\n",
    "\n",
    "# check if the test sets are the same\n",
    "(y_test == y_t_test).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, we will perform the comparison of the same model fitted on both of the datasets. In our exercise, we will use an `SGDClassifier()` which is simply a linear classifier who tries to find a straight line to separate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our model\n",
    "linear_model = sklearn.linear_model.SGDClassifier(random_state=SEED)\n",
    "\n",
    "# fit the model on the training set of the original dataset\n",
    "linear_model.fit(X_train, y_train)\n",
    "# compute the train accuracy\n",
    "acc_train = sklearn.metrics.accuracy_score(linear_model.predict(X_train), y_train)\n",
    "print('Accuracy (train) on the original data: %0.2f' % acc_train)\n",
    "# compute the train accuracy\n",
    "acc_test = sklearn.metrics.accuracy_score(linear_model.predict(X_test), y_test)\n",
    "print('Accuracy (test) on the original data: %0.2f' % acc_test)\n",
    "print('-' * 40)\n",
    "\n",
    "# fit the model on the training set of the transformed dataset\n",
    "linear_model.fit(X_t_train, y_t_train)\n",
    "# compute the train accuracy\n",
    "acc_t_train = sklearn.metrics.accuracy_score(linear_model.predict(X_t_train), y_t_train)\n",
    "print('Accuracy (train) on the transformed data: %0.2f' % acc_t_train)\n",
    "# compute the train accuracy\n",
    "acc_t_test = sklearn.metrics.accuracy_score(linear_model.predict(X_t_test), y_t_test)\n",
    "print('Accuracy (test) on the transformed data: %0.2f' % acc_t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the comparison, we get approximately a 5-10% performance gain when using the dataset consisting of the engineered features, which is quite a lot considering the small effort for it. Therefore it is always a good idea to spend some time looking at the data and thinking of what features might be useful and can be created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering with the kernel trick\n",
    "In this part of the exercise, we will look at feature engineering using the kernel trick, which is very useful if your data is not linearly separable in the current dimension of your dataset.\n",
    "\n",
    "For this example, we will use a generated gaussian dataset (`make_gaussian_quantiles()`), which consists of two features and is, therefore, a 2D dataset. This dataset is not linearly separable as we will see shortly when we plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate our dataset\n",
    "X, y = sklearn.datasets.make_gaussian_quantiles(n_samples=200, n_classes=2, random_state=SEED)\n",
    "\n",
    "# split the data into train and test set\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot our dataset and can clearly see that it is not linearly separable, since all the points of one class are arranged in an ellipse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1], s=60, c=y_train, cmap='bwr')\n",
    "plt.xlabel('feature X0')\n",
    "plt.ylabel('feature X1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will train a linear model on the dataset. This shows clearly that a linear model is not able to separate the data with a line. It also shows that the model is only able to guess whether the point belongs to one class or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = sklearn.linear_model.SGDClassifier()\n",
    "# train the model\n",
    "linear_model.fit(X_train, y_train)\n",
    "# train accuracy\n",
    "acc_train = sklearn.metrics.accuracy_score(linear_model.predict(X_train), y_train)\n",
    "print('Accuracy (train): %0.2f' % acc_train)\n",
    "# test accuracy\n",
    "acc_test = sklearn.metrics.accuracy_score(linear_model.predict(X_test), y_test)\n",
    "print('Accuracy (test): %0.2f' % acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the kernel trick to project the dataset onto a higher dimensional space, by combining the features. In this case, we will project the dataset onto a 3D space, by combining the features as follows:\n",
    "\n",
    "$$ x_0, x_1 \\rightarrow x_0, x_1, x_0^2 + x_1^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the training set\n",
    "X_train_transformed = np.asarray([[x[0], x[1], x[0]**2 + x[1]**2] for x in X_train])\n",
    "# transform the test set\n",
    "X_test_transformed = np.asarray([[x[0], x[1], x[0]**2 + x[1]**2] for x in X_test])\n",
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be visualized using a 3D scatter plot. This already shows that the blue points are now much better separable from the red ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(X_train_transformed[:, 0], \n",
    "              X_train_transformed[:, 1], \n",
    "              X_train_transformed[:, 2], \n",
    "              s=60, c=y_train, cmap='bwr')\n",
    "ax.set_xlabel('feature X0')\n",
    "ax.set_ylabel('feature X1')\n",
    "ax.set_zlabel('feature X2')\n",
    "ax.view_init(3, 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, we will fit our linear model again and compare the performance with the original dataset. We can see that with this simple trick, we transformed our dataset from a not linearly separable dataset to a somewhat separable one and got us a 25% performance gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "linear_model.fit(X_train_transformed, y_train)\n",
    "# train accuracy\n",
    "acc_train = sklearn.metrics.accuracy_score(linear_model.predict(X_train_transformed), y_train)\n",
    "print('Accuracy (train): %0.2f' % acc_train)\n",
    "# test accuracy\n",
    "acc_test = sklearn.metrics.accuracy_score(linear_model.predict(X_test_transformed), y_test)\n",
    "print('Accuracy (test): %0.2f' % acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering for image data\n",
    "This section illustrates feature engineering for image data. One of the most important image features are edges. Thus a simple edge detection is implemented.\n",
    "To illustrate the process, an image is first loaded and then converted to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = skimage_data.astronaut()\n",
    "grayscale = rgb2gray(original)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(original)\n",
    "ax[0].set_title(\"Original\")\n",
    "ax[1].imshow(grayscale, cmap=plt.cm.gray)\n",
    "ax[1].set_title(\"Grayscale\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest ways to detect edges in an image is to subtract a shifted image from the original one. This will already result in the edges of the image. Depending on in which direction the image is shifted, we get edges of the $x$ or $y$ direction.\n",
    "\n",
    "The function `shift_image` implements this simple behaviour. Since the image will be shifted by a certain amount of pixel, we'll need to define with what the empty pixels are filled. Here `0` is used for this. \n",
    "\n",
    "To calculate the gradient of the image, it will need to be shifted in both direction ($x$ and $y$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_image(X, dx, dy):\n",
    "    X = np.roll(X, dy, axis=0)\n",
    "    X = np.roll(X, dx, axis=1)\n",
    "    if dy>0:\n",
    "        X[:dy, :] = 0\n",
    "    elif dy<0:\n",
    "        X[dy:, :] = 0\n",
    "    if dx>0:\n",
    "        X[:, :dx] = 0\n",
    "    elif dx<0:\n",
    "        X[:, dx:] = 0\n",
    "    return X\n",
    "\n",
    "shifted_x = shift_image(grayscale, 4, 0)\n",
    "shifted_y = shift_image(grayscale, 0, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the edges for either direction, the two images can now be subtracted. To then calculate the gradient magnitude, the euclidean norm is calculated from the sum of the subtracted images.\n",
    "\n",
    "$$ \\text{gradient magnitude} = |\\nabla f(x,y)| = \\sqrt{f_x^2 + f_y^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract the shifted image\n",
    "subtracted_x = np.subtract(grayscale, shifted_x)\n",
    "subtracted_y = np.subtract(grayscale, shifted_y)\n",
    "\n",
    "# calculate the gradient formula with the given formula\n",
    "gradient_subtracted = np.sqrt(subtracted_x**2 + subtracted_y**2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 4))\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(grayscale, cmap=plt.cm.gray)\n",
    "ax[0].set_title(\"Original\")\n",
    "\n",
    "ax[1].imshow(subtracted_x, cmap=plt.cm.gray)\n",
    "ax[1].set_title(\"Subtracted by shift in x-direction\")\n",
    "\n",
    "ax[2].imshow(subtracted_y, cmap=plt.cm.gray)\n",
    "ax[2].set_title(\"Subtracted by shift in y-direction\")\n",
    "\n",
    "ax[3].imshow(gradient_subtracted, cmap=plt.cm.gray)\n",
    "ax[3].set_title(\"Gradient magnitude\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same result, can be obtained by convolving the image with a specific kernel (`kernel_x` and `kernel_y`). For example the `kernel_y` will subtract the pixel above (`-1`) from the current one (`1`). This will be applied to the entire image by shifting it over the image. After the edges in both $x$ and $y$ direction have been obtained, the same formula as above can be used to calculate the gradient magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothen the image to get better edges\n",
    "grayscale = scipy.ndimage.filters.gaussian_filter(grayscale, 2)\n",
    "\n",
    "# define the kernels for the edge detection\n",
    "kernel_x = np.array([[-1, 1]])\n",
    "kernel_y = np.array([[-1], [1]])\n",
    "\n",
    "# convolve the image with the kernel\n",
    "convolved_x = scipy.ndimage.convolve(grayscale, kernel_x)\n",
    "convolved_y = scipy.ndimage.convolve(grayscale, kernel_y)\n",
    "\n",
    "# calculate the gradient magnitude\n",
    "gradient_convolved = np.sqrt(convolved_x**2 + convolved_y**2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 4))\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(grayscale, cmap=plt.cm.gray)\n",
    "ax[0].set_title(\"Original\")\n",
    "\n",
    "ax[1].imshow(convolved_x, cmap=plt.cm.gray)\n",
    "ax[1].set_title(\"Convolved with x-kernel\")\n",
    "\n",
    "ax[2].imshow(convolved_y, cmap=plt.cm.gray)\n",
    "ax[2].set_title(\"Convolved with y-kernel\")\n",
    "\n",
    "ax[3].imshow(gradient_convolved, cmap=plt.cm.gray)\n",
    "ax[3].set_title(\"Gradient magnitude\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Feature Engineering for stock price data\n",
    "In the last part of the exercise, we will look at feature engineering for stock price data and will use the yahoo finance dataset. From that, we will take a look at the NASDAQ stock.\n",
    "\n",
    "As a first step, we will define the start and end date of the stock data we want to receive. We will here use 20 years of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.datetime(2000, 1, 1)\n",
    "end_date = datetime.datetime(2020, 12, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will download the data using the `yfinance` package, which provides a convenient way of downloading financial data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download the stock price data\n",
    "try:\n",
    "    stock = yf.download('NDAQ', start=start_date, end=end_date, progress=True)\n",
    "except Exception:\n",
    "    raise Exception('Failed to download data.')\n",
    "# plot the data\n",
    "stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we will plot the data and look at it. We can see that the stock is increasing pretty nicely. But since financial data is tough to work with, each additional feature wich helps the learning algorithm to understand the stock is welcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "stock['Adj Close'].plot()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Adjusted')\n",
    "plt.title('NASDAQ price data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will construct the daily return of the stock, which can be done by taking today's value and the one from tomorrow and dividing them. Then you only need to subtract $1$ from it, and you've got the daily returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasdaq_daily_returns = (stock['Adj Close'] / stock['Adj Close'].shift(1)) - 1\n",
    "nasdaq_daily_returns = nasdaq_daily_returns.to_frame()\n",
    "nasdaq_daily_returns.columns = ['Simple Dailiy Return']\n",
    "\n",
    "# join the dataframes\n",
    "stock = stock.join(nasdaq_daily_returns)\n",
    "stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will plot the newly created feature and see that the daily returns give us already some additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "stock['Simple Dailiy Return'].plot()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Percent')\n",
    "plt.title('NASDAQ dailiy return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also compute the volatility of the stock, by using the daily returns from before, which gives us even more insight about the stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add our new computed feature to the dataframe\n",
    "stock['volatility'] = abs(stock['Simple Dailiy Return']).rolling(22).mean()\n",
    "stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we now plot both the absolute value of the daily returns and the volatility of the stock price, we can see how the stock has performed over time on average over 22 days. Both of these features could now be used to give a learning algorithm some additional insights about the data, which is not obvious from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(abs(stock['Simple Dailiy Return']))\n",
    "plt.plot(stock['volatility'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.title('NASDAQ volatility')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
