{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVMs) are a powerful supervised learning algorithm used for both **classification** and **regression**. SVMs establish a hyperplane that separates the two classes by maximizing the margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import random\n",
    "import seaborn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_blobs, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "seaborn.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit a support vector machine with a linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the decision boundary. In the following plot the dashed lines touch the *support vectors*, which are stored in the ``support_vectors_`` attribute of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(clf, ax=None):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    x = np.linspace(plt.xlim()[0], plt.xlim()[1], 30)\n",
    "    y = np.linspace(plt.ylim()[0], plt.ylim()[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    P = np.zeros_like(X)\n",
    "    for i, xi in enumerate(x):\n",
    "        for j, yj in enumerate(y):\n",
    "            P[i, j] = clf.decision_function([[xi, yj]])\n",
    "    # plot the margins\n",
    "    ax.contour(X, Y, P, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "    \n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
    "plot_svc_decision_function(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We additoonally highlight the support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=200, facecolors='none',edgecolors=\"black\");\n",
    "plot_svc_decision_function(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset above was non-overlapping (or linearly separable), which means we could come up with a hyperplane that separated the dataset perfectly. Let us now consider a dataset where no perfect separation is possible. In this case the SVM tries to minimize the datapoints lying on the wrong side of the hyperplane. These datapoints are considered support vectors as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we generate the datapoints of the first class by sampling from a normal distribution with standard deviation 1.3 and mean (2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_entries=100\n",
    "X=np.zeros((2*num_entries,2))\n",
    "\n",
    "for i in range(0,num_entries):\n",
    "    X[i,0]=np.random.normal()*1.3+2\n",
    "    X[i,1]=np.random.normal()*1.3+4\n",
    "y = num_entries*[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we sample the data points from the second class with standard deviation 1.0 and mean (1,0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "for i in range(num_entries,2*num_entries):\n",
    "    X[i,0]=np.random.normal()+1\n",
    "    X[i,1]=np.random.normal()\n",
    "y2 = num_entries*[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us combine the class vectors `y` and `y2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.extend(y2)\n",
    "\n",
    "assert len(X) == len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
    "_ = plt.xlim(-1, 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit a linear SVM to find the best separating hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us again visualize the hyperplane and the support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=200, facecolors='none',edgecolors=\"black\");\n",
    "plot_svc_decision_function(clf)\n",
    "_ = plt.xlim(-1, 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine with Kernels\n",
    "\n",
    "Kernels are useful when the decision boundary is not linear. A Kernel is a similarity measure of two data points after projection to some higher dimensional space. Let us generate a data set that is even less linearly separable than the one before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_circles, y_circles = make_circles(100, factor=.1, noise=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and visualize  a linear SVM and fit it to X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear').fit(X_circles, y_circles)\n",
    "\n",
    "plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, s=50, cmap='spring')\n",
    "plot_svc_decision_function(clf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel called **radial basis function (rbf)** will do the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf')\n",
    "clf.fit(X_circles, y_circles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, s=50, cmap='spring')\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=200, facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skin Disease Dataset\n",
    "\n",
    "We want to apply the SVM to segment skin diseases. Each row is an image pixel to which 14 different image filters have been applied (feature engineering, column t0 to t13). The class (target variable) indidates whether the pixel shows healthy skin or a skin disease (labels from medical doctors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"skin_disease.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs are not very fast. In order to save time, we only use 50000 entries for training and validation. We also display a histogram of the target variable and observe that the data is extremely disbalanced. This is why we will use the f1-score for performance measurement below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(10000)\n",
    "\n",
    "_ = df['class'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us split this dataset into training and validation set (we do not need a test set here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(df, test_size=0.5)\n",
    "\n",
    "X_train = train.drop('class', axis=1)\n",
    "X_valid = valid.drop('class', axis=1)\n",
    "\n",
    "y_train = train[\"class\"]\n",
    "y_valid = valid[\"class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train and evaluate an SVM classifier on this dataset, which can take some minutes. \n",
    "Let us first use the `rbf` kernel and a `gamma` value of 0.1.\n",
    "We measure the f1-score and accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'SVM with RBF kernel' : SVC(kernel='rbf', gamma=0.1),\n",
    "    'SVM with linear kernel' : SVC(),\n",
    "    'Decision Tree' : DecisionTreeClassifier(max_depth=5)\n",
    "}\n",
    "\n",
    "\n",
    "for name, model in classifiers.items():\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    \n",
    "    print (\"Performance of {} is {:.3f}:\".format(name, f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground for Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0,0,-1], [3,0,-1], [0,2,1], [2,3,1]]\n",
    "df = pd.DataFrame(data, columns=['x', 'y', 'label'])\n",
    "\n",
    "_ = plt.scatter(df['x'], df['y'], c=df['label'], s=50, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear').fit(df[['x', 'y']].values, df['label'])\n",
    "\n",
    "plt.scatter(df['x'], df['y'], c=df['label'], s=50, cmap='rainbow')\n",
    "plt.quiver([0], [1], [0], [1], angles='xy', scale_units='xy', scale=1)\n",
    "plot_svc_decision_function(clf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5,5,100)\n",
    "\n",
    "plt.scatter(df['x'], df['y'], c=df['label'], s=50, cmap='rainbow')\n",
    "plt.plot(x, 1/3 * x + 1, '-r', label='y = 1/3 * x + 1')\n",
    "plt.plot(x, 0*x + 1, '-b', label='y = 1')\n",
    "plt.quiver([0,0], [1,1], [0,1/3], [1,-1], angles='xy', scale_units='xy', scale=1)\n",
    "\n",
    "plt.axis((-5,5,-5,5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print (\"f1 SVM:\", f1)\n",
    "print (\"accuracy SVM:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.2",
    "jupytext_version": "0.8.6"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
