{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import sklearn\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "from ipywidgets import interact\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - 1D Toy example\n",
    "Consider the case where random numbers are created by two different Gaussian distributions with identical variance. We also know the labels from which distribution each number was originating from. The generated data could, for example, represent how many days a student has learned for the ML exam and the target variable is if they have passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_passed = np.random.normal(5,0.7,100)\n",
    "students_passed[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_failed = np.random.normal(2,0.7,100)\n",
    "students_failed[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this data for a logistic regression model, we combine the vectors $\\text{students_passed}$ and $\\text{students_failed}$ into a vector $X$ and create the corresponding labels $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label: failed\n",
    "zeros = [0]*100\n",
    "# label: passed\n",
    "ones = [1]*100\n",
    "\n",
    "X = np.concatenate((students_passed, students_failed))\n",
    "y = np.concatenate((ones, zeros))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot both type of points in a scatter plot, where the points generated by the first distribution are plotted in blue have the label $y=0$, while the points of the second distribution are plotted in orange at $y=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_map = {0: 'failed', 1: 'passed'}\n",
    "ax = sns.scatterplot(X, y, hue=pd.Series(y).map(legend_map))\n",
    "ax.set_xlabel('days spent learning for the ML exam')\n",
    "ax.set_ylabel('if students passed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to determine, if an arbitrary previously unseen point belongs rather to distribution 1 or two distribution 2. For that, we want to employ logistic regression. Similar to linear regression, we first consider a model with a single independent variable and two parameters $\\theta_0$ and $\\theta_1$.\n",
    "\n",
    "The probability, that $x$ belongs to either of the two classes is determined using the sigmoid function.\n",
    "\n",
    "$$\n",
    "  \\sigma(x) = \\frac{1}{1+e^{-(\\theta_0 + \\theta_1x)}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implement the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # START YOUR CODE\n",
    "    \n",
    "    # END YOUR CODE\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Click on the dots to display the solution*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "solution2": "hidden",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-8, 8)\n",
    "plt.plot(x, sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Assisgnment - Verifciation on Ilias**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implement the `predict` function. On **Ilias**, report the i=7 entry of y_pred below and check if your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def predict(X, theta0, theta1):\n",
    "    # START YOUR CODE\n",
    "\n",
    "    # END YOUR CODE\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = 1.0\n",
    "theta1 = 1.0\n",
    "\n",
    "y_pred = predict(X, theta0, theta1)\n",
    "y_pred[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The value below is the answer for the Ilias Quiz \"05A Supervised Learning: Classification\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Decision Boundary\n",
    "The decision boundary is given by the x such that: $-\\theta_0-\\theta_1 x=0$.\n",
    "\n",
    "We can solve this equation for x: $x=-\\frac{\\theta_0}{\\theta_1}$\n",
    "Now let us plot the decision boundary and the logistic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, theta0, theta1, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    x = np.arange(X.min()-1, X.max()+1, 0.01).reshape(-1,1)\n",
    "    y_pred = predict(x, theta0, theta1)\n",
    "    ax.plot(x, y_pred, color=\"r\")\n",
    "    ax.axvline(-theta0/theta1, color=\"g\")\n",
    "    ax.set_title(\"Decision Boundary\")\n",
    "    \n",
    "legend_map = {0: 'failed', 1: 'passed'}\n",
    "ax = sns.scatterplot(X, y, hue=pd.Series(y).map(legend_map))\n",
    "ax.set_xlabel('days spent learning for the ML exam')\n",
    "ax.set_ylabel('if students passed')\n",
    "plot_decision_boundary(X, theta0, theta1, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "The cross-entropy cost function $J(\\boldsymbol\\theta)$, where $\\boldsymbol\\theta=\\left(\\theta_0,\\theta_1\\right)$ is given by\n",
    "\n",
    "$$\n",
    "    J(\\boldsymbol\\theta) =\n",
    "      - \\frac{1}{n} \\sum_{i=1}^n%\n",
    "        \\left[y_i\\log h(\\boldsymbol\\theta,\\mathbf{X_i})\n",
    "            + (1-y_i)\\log\\left(\n",
    "               1-h(\\boldsymbol\\theta,\\mathbf{X_i})\\right)\\right]\n",
    "$$\n",
    "\n",
    "where $h(\\boldsymbol\\theta,\\mathbf{X_i})=\\sigma\\left(\\mathbf{X_i}^T\\boldsymbol\\theta\\right)=\\sigma\\left(\\theta_0+\\theta_1 x\\right)$ and $\\sigma$ is the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implement the cost function. Verify your code by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def cost_function(y, y_pred):\n",
    "    # START YOUR CODE\n",
    "    \n",
    "    \n",
    "    # END YOUR CODE\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Click on the dots to display the solution*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "solution2": "hidden",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cost_function(y, y_pred):\n",
    "    n = y.shape[0]\n",
    "    cost = -(1.0 / n) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should be able to run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_assert = np.array([1, 0, 0])\n",
    "y_pred = np.array([0.98, 0.2, 0.6])\n",
    "\n",
    "expected_cost = 0.38654566350196135\n",
    "actual_cost = cost_function(y_assert, y_pred)\n",
    "\n",
    "np.testing.assert_almost_equal(actual_cost, expected_cost, decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients\n",
    "For applying gradient descent, we define the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y, theta0, theta1):\n",
    "    y_pred = predict(X, theta0, theta1)\n",
    "    diff = y_pred - y\n",
    "    \n",
    "    n = len(X)\n",
    "    grad_theta0 = np.sum(diff) / n\n",
    "    grad_theta1 = np.dot(diff, X.T) / n\n",
    "    \n",
    "    return grad_theta0, grad_theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "Now we are ready to determine the optimal values for the parameters $\\theta_0$ and $\\theta_1$ using the gradient descent algorithm.\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{Repeat}\\;\\mathrm{(until}\\;\\mathrm{convergence)} \\left\\{\\right.\n",
    "  \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\\\\n",
    "      \\boldsymbol\\theta_{k+1} = \\boldsymbol\\theta_{k}-\\alpha\\frac{1}{n}\\sum_{i=1}^n\n",
    "          \\left(h(\\boldsymbol\\theta_k,\\mathbf{x}^{(i)})-y^{(i)}\\right)\\mathbf{x}^{(i)},\n",
    "          \\quad k = 0,\\,1,\\,2,\\,3,\\,\\ldots,\\mathtt{kmax}\\\\\n",
    "    \\left.\\right\\}\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "> Implement the `fit` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def fit(X, y, alpha, num_epochs, display_every=10):\n",
    "    theta0 = 0.0\n",
    "    theta1 = np.random.randn()\n",
    "    \n",
    "    hist = defaultdict(list)\n",
    "    for i in tqdm(range(num_epochs)):\n",
    "        # START YOUR CODE\n",
    "        # calculate gradients\n",
    "        \n",
    "        \n",
    "        # update model parameters theta0 and theta1\n",
    "        \n",
    "        \n",
    "        # calculate the current costs\n",
    "        \n",
    "        \n",
    "        # END YOUR CODE\n",
    "        grad_theta0, grad_theta1 = gradient(X, y, theta0, theta1)\n",
    "        theta0 = theta0 - alpha * grad_theta0\n",
    "        theta1 = theta1 - alpha * grad_theta1\n",
    "        \n",
    "        y_pred = predict(X, theta0, theta1)\n",
    "        curr_cost = cost_function(y, y_pred)\n",
    "        \n",
    "        hist[\"cost\"].append(curr_cost)\n",
    "        hist[\"theta0\"].append(theta0)\n",
    "        hist[\"theta1\"].append(theta1)\n",
    "        \n",
    "        if i == 0 or (i+1) % display_every == 0:\n",
    "            print(\"Epoch {} -  cost: {}\".format(i+1, curr_cost))\n",
    "        \n",
    "    return theta0, theta1, hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Click on the dots to display the solution*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "solution2": "hidden",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(X, y, alpha, num_epochs, display_every=10):\n",
    "    theta0 = 0.0\n",
    "    theta1 = np.random.randn()\n",
    "    \n",
    "    hist = defaultdict(list)\n",
    "    for i in tqdm(range(num_epochs)):\n",
    "        # calculate gradients\n",
    "        grad_theta0, grad_theta1 = gradient(X, y, theta0, theta1)\n",
    "        \n",
    "        # update model parameters theta0 and theta1\n",
    "        theta0 = theta0 - alpha * grad_theta0\n",
    "        theta1 = theta1 - alpha * grad_theta1\n",
    "        \n",
    "        # calculate the current costs\n",
    "        y_pred = predict(X, theta0, theta1)\n",
    "        curr_cost = cost_function(y, y_pred)\n",
    "        \n",
    "        hist[\"cost\"].append(curr_cost)\n",
    "        hist[\"theta0\"].append(theta0)\n",
    "        hist[\"theta1\"].append(theta1)\n",
    "        \n",
    "        if i == 0 or (i+1) % display_every == 0:\n",
    "            print(\"Epoch {} -  cost: {}\".format(i+1, curr_cost))\n",
    "        \n",
    "    return theta0, theta1, hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot validation curve\n",
    "We implement a function that allows us to plot the validation curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_validation_curve(costs, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.set_ylabel(\"Cost\")\n",
    "    ax.set_title(\"Validation Curve\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.plot(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run gradient descent algorithm\n",
    "Let's run the gradient descent algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "num_epochs = 10000\n",
    "\n",
    "theta0, theta1, hist = fit(X, y, alpha, num_epochs, display_every=1000)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(15,4))\n",
    "\n",
    "# scatter plot\n",
    "legend_map = {0: 'failed', 1: 'passed'}\n",
    "ax[0] = sns.scatterplot(X, y, hue=pd.Series(y).map(legend_map), ax=ax[0])\n",
    "ax[0].set_xlabel('days spent learning for the ML exam')\n",
    "ax[0].set_ylabel('if students passed')\n",
    "plot_decision_boundary(X, theta0, theta1, ax[0])\n",
    "\n",
    "# validation curve\n",
    "plot_validation_curve(hist[\"cost\"], ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Learning \n",
    "Let's visualize how the decision boundary changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(epoch=(0, len(hist[\"theta0\"]), 100))\n",
    "def visualize_learning(epoch=100):\n",
    "    legend_map = {0: 'failed', 1: 'passed'}\n",
    "    ax = sns.scatterplot(X, y, hue=pd.Series(y).map(legend_map))\n",
    "    ax.set_xlabel('days spent learning for the ML exam')\n",
    "    ax.set_ylabel('if students passed')\n",
    "    if epoch == 0:\n",
    "        epoch += 1\n",
    "    plot_decision_boundary(X, hist[\"theta0\"][epoch-1], hist[\"theta1\"][epoch-1], ax)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "Let's calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(X, theta0, theta1)\n",
    "y_pred[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We label a point as 1 if the predicted value is larger than 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# y_pred_class = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Click on the dots to display the solution*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "solution2": "hidden",
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_class = y_pred > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y, y_pred_class)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Multiple Logistic Regression - Toy example\n",
    "In the second part, logistic regression is used in a 2D toy example. Here the data is loaded from a `.csv` file, but it was also generated artificially for illustration purposes. Here the data can, for example, correspond to\n",
    "* feature 1: days spent learning for the ML exam\n",
    "* feature 2: days spent working in the ML domain (prior experience)\n",
    "* target variable: if students have passed the exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"classification_data.csv\", header=None)\n",
    "df.columns = ['days spent learning', 'prior experience', 'exam passed']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "X_2d = df.values[:, 0:2]\n",
    "y_2d = df.values[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_2d, y_2d, test_size=0.1)\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict function\n",
    "The first step is to modify our `predict` function to handle multiple thetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def predict(X, bias, thetas):\n",
    "    # START YOUR CODE\n",
    "    \n",
    "    \n",
    "    # END YOUR CODE\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Click on the dots to display the solution*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "solution2": "hidden",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(X, bias, thetas):\n",
    "    z = bias + np.dot(X, thetas)\n",
    "    y_pred = sigmoid(z)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient function\n",
    "Let's modify the `gradient` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def gradient(X, y, bias, thetas):\n",
    "    # START YOUR CODE\n",
    "    \n",
    "    \n",
    "    \n",
    "    # END YOUR CODE\n",
    "    return grad_bias, grad_thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Click on the dots to display the solution*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "solution2": "hidden",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient(X, y, bias, thetas):\n",
    "    y_pred = predict(X, bias, thetas)\n",
    "    diff = y_pred - y\n",
    "    \n",
    "    n = len(X)\n",
    "    grad_bias = np.sum(diff) / n\n",
    "    grad_thetas = np.dot(diff, X) / n\n",
    "    \n",
    "    return grad_bias, grad_thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, alpha, num_epochs, display_every=100):\n",
    "    bias = 0.0\n",
    "    thetas = np.random.randn(*(1, X.shape[1])).reshape(-1)\n",
    "        \n",
    "    hist = defaultdict(list)\n",
    "    for epoch in tqdm(range(1, num_epochs+1)):\n",
    "        # calculate gradients\n",
    "        grad_bias, grad_thetas = gradient(X, y, bias, thetas)\n",
    "        \n",
    "        # update model parameters\n",
    "        bias = bias - alpha * grad_bias\n",
    "        thetas = thetas - alpha * grad_thetas\n",
    "        \n",
    "        # calculate the current costs\n",
    "        y_pred = predict(X, bias, thetas)\n",
    "        curr_cost = cost_function(y, y_pred)\n",
    "        \n",
    "        hist[\"cost\"].append(curr_cost)\n",
    "        \n",
    "        if epoch % display_every == 0:\n",
    "            print(\"Epoch {} -  cost: {}\".format(epoch, curr_cost))\n",
    "        \n",
    "    return bias, thetas, hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Gradient Descent\n",
    "> Apply the gradient descent algorithm. Use the learning rate 0.1. Plot the validation curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Click on the dots to display the solution*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "solution2": "hidden",
    "tags": []
   },
   "outputs": [],
   "source": [
    "bias_2d, thetas_2d, hist_2d = fit(X_train, y_train, alpha=0.1, num_epochs=10000, display_every=1000)\n",
    "plot_validation_curve(hist_2d[\"cost\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"decision boundary: %.3f + %.3f * x1 + %.3f * x2 = 0\"\n",
    "      % (bias_2d, thetas_2d[0], thetas_2d[1]))\n",
    "\n",
    "x1 = np.array(X_train[:,0].T)\n",
    "x2 = np.array(X_train[:,1].T)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
    "ax.set_xlabel('days spent learning')\n",
    "ax.set_ylabel('prior knowledge (in days)')\n",
    "color = ['blue' if l == 0 else 'green' for l in y_train]\n",
    "scat = ax.scatter(x1, x2, color=color)\n",
    "\n",
    "# inline function for decision boundary (unless vertical)\n",
    "y_ = lambda x: ((-1)*(bias_2d + thetas_2d[0]*x) / thetas_2d[1])\n",
    "\n",
    "def plot_line(y, data_pts):\n",
    "    x_vals = [i for i in\n",
    "              range(int(min(data_pts)-1),\n",
    "                    int(max(data_pts))+2)]\n",
    "    y_vals = [y(x) for x in x_vals]\n",
    "    plt.plot(x_vals,y_vals, 'r')\n",
    "\n",
    "plot_line(y_, x1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "How should we evaluate our result? Of course this is highly dependent on both our original business problem and the data at hand. Questions such as\n",
    "* Does the evaluation result need to be explainable to management, without using formulas and technical terms?\n",
    "* Do we have a high class imbalance?\n",
    "* Are False Positives and False Negatives equally bad? Does one of the two incur a high cost for our business and needs to be avoided?\n",
    "* How do we rate the confidence? Do we want to penalise a classifier when it classifies a sample wrongly but is very sure of this result?\n",
    "\n",
    "We will look at the metrics Accuracy and F1-Score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Predict the data on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# y_pred = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Click on the dots to display the solution*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "solution2": "hidden",
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = predict(X_test, bias_2d, thetas_2d)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "First we compute and plot the confusion matrix using the utility methods `compute_confusion_matrix` and `plot_confusion_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(true, pred):\n",
    "    # number of classes\n",
    "    K = len(np.unique(true))\n",
    "    c_mat = np.zeros((K, K))\n",
    "    \n",
    "    for i in range(len(true)):\n",
    "        c_mat[int(true[i])][int(pred[i])] += 1\n",
    "        \n",
    "    return c_mat\n",
    "\n",
    "def plot_confusion_matrix(cm):\n",
    "    fig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\n",
    "    sns.heatmap(cm, \n",
    "                xticklabels=['True', 'False'],\n",
    "                yticklabels=['True', 'False'],\n",
    "                annot=True,ax=ax1,\n",
    "                linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\n",
    "    plt.title('Confusion Matrix', fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# cm = ...\n",
    "# plot..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Click on the dots to display the solution*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "solution2": "hidden",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm = compute_confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Finally calculate and print the accuracy and the f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scores(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Extracts the tp, tn, fp, fn from the\n",
    "    confusion matrix.\n",
    "    \"\"\"\n",
    "    # true positive\n",
    "    tp = confusion_matrix[0][0]\n",
    "    # true negative\n",
    "    tn = confusion_matrix[1][1]\n",
    "    # false positive\n",
    "    fp = confusion_matrix[0][1]\n",
    "    # false negative\n",
    "    fn = confusion_matrix[1][0]\n",
    "    \n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def accuracy_score(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Computes the accuracy from a confusion matrix.\n",
    "    \"\"\"\n",
    "    tp, tn, fp, fn = extract_scores(confusion_matrix)\n",
    "    acc = (tp + tn)/np.sum(confusion_matrix)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def f1_score(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Computes the f1 score from a confusion matrix.\n",
    "    \"\"\"\n",
    "    tp, tn, fp, fn = extract_scores(confusion_matrix)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1 = (2*precision*recall)/(precision+recall)\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# accuracy = ...\n",
    "# f1 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Click on the dots to display the solution*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "solution2": "hidden",
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(cm)\n",
    "f1 = f1_score(cm)\n",
    "\n",
    "print (\"test accuracy: %.2f\" % accuracy)\n",
    "print (\"test f1 score: %.2f\" % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Assignment\n",
    "> Solve the following Programming assignment and check your solution in the Illias Quiz **05A Supervised Learning: Classification - Notebook Verification**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous examples you implemented Logistic Regression from scratch. Now you are going to repeat the calulcations using scikit-learn's implementation of Logistic Regression. Use the data of the Multiple Logistic Regression example also using the identical train/test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Logistic Regression, what is the score on the test set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the standard metric of the score function of scikitlearn's implementation of LogisticRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your answers in the Ilias Quiz."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
